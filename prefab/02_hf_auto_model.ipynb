{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89eb0ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07f6dfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/muhsinmohammed/miniconda3/envs/dev_base/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5706e32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my-bert-model/tokenizer_config.json',\n",
       " './my-bert-model/special_tokens_map.json',\n",
       " './my-bert-model/vocab.txt',\n",
       " './my-bert-model/added_tokens.json',\n",
       " './my-bert-model/tokenizer.json')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./my-bert-model\")\n",
    "tokenizer.save_pretrained(\"./my-bert-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eeff0998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 876288\n",
      "-rw-r--r--@ 1 muhsinmohammed  1028238298   695K Jun 27 12:51 tokenizer.json\n",
      "-rw-r--r--@ 1 muhsinmohammed  1028238298   226K Jun 27 12:51 vocab.txt\n",
      "-rw-r--r--@ 1 muhsinmohammed  1028238298   125B Jun 27 12:51 special_tokens_map.json\n",
      "-rw-r--r--@ 1 muhsinmohammed  1028238298   1.2K Jun 27 12:51 tokenizer_config.json\n",
      "-rw-r--r--@ 1 muhsinmohammed  1028238298   418M Jun 27 12:51 model.safetensors\n",
      "-rw-r--r--@ 1 muhsinmohammed  1028238298    90B Jun 27 12:51 generation_config.json\n",
      "-rw-r--r--@ 1 muhsinmohammed  1028238298   676B Jun 27 12:51 config.json\n"
     ]
    }
   ],
   "source": [
    "! ls -lth ./my-bert-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab9fc89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = AutoModelForMaskedLM.from_pretrained(\"./my-bert-model\")\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(\"./my-bert-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1406766a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 109,514,298 parameters\n",
      "Hidden size: 768\n",
      "Number of layers: 12\n",
      "Number of attention heads: 12\n",
      "Vocabulary size: 30522\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Model size: {model.num_parameters():,} parameters\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Number of layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Number of attention heads: {model.config.num_attention_heads}\")\n",
    "print(f\"Vocabulary size: {model.config.vocab_size}\")\n",
    "\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35b6e3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 11])\n",
      "Mask token index: tensor([7])\n",
      "Mask logits: tensor([[-2.6876, -3.2343, -3.1478,  ..., -2.5129, -3.2148, -5.6780]])\n",
      "[4628]\n",
      "Predicted word: passenger\n"
     ]
    }
   ],
   "source": [
    "text = \"Airbus A330 is a [MASK] aircraft.\"\n",
    "inputs = local_tokenizer(text, return_tensors=\"pt\")\n",
    "# print(inputs)\n",
    "print('Input shape:', inputs['input_ids'].shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = local_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# logits.shape\n",
    "\n",
    "mask_token_index = (inputs.input_ids == local_tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "print(\"Mask token index:\", mask_token_index)\n",
    "\n",
    "mask_logits = logits[0, mask_token_index, :]\n",
    "\n",
    "print(\"Mask logits:\", mask_logits)\n",
    "\n",
    "top_token = torch.topk(mask_logits, k=1, dim=1).indices[0].tolist()\n",
    "print(top_token)\n",
    "word = local_tokenizer.decode(top_token)\n",
    "print(f\"Predicted word: {word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00af24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertForMaskedLM"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(local_model) ## transformers.models.bert.modeling_bert.BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "215b2d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fec198d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'return_dict': True,\n",
       " 'output_hidden_states': False,\n",
       " 'output_attentions': False,\n",
       " 'torchscript': False,\n",
       " 'torch_dtype': None,\n",
       " 'use_bfloat16': False,\n",
       " 'tf_legacy_loss': False,\n",
       " 'pruned_heads': {},\n",
       " 'tie_word_embeddings': True,\n",
       " 'chunk_size_feed_forward': 0,\n",
       " 'is_encoder_decoder': False,\n",
       " 'is_decoder': False,\n",
       " 'cross_attention_hidden_size': None,\n",
       " 'add_cross_attention': False,\n",
       " 'tie_encoder_decoder': False,\n",
       " 'max_length': 20,\n",
       " 'min_length': 0,\n",
       " 'do_sample': False,\n",
       " 'early_stopping': False,\n",
       " 'num_beams': 1,\n",
       " 'num_beam_groups': 1,\n",
       " 'diversity_penalty': 0.0,\n",
       " 'temperature': 1.0,\n",
       " 'top_k': 50,\n",
       " 'top_p': 1.0,\n",
       " 'typical_p': 1.0,\n",
       " 'repetition_penalty': 1.0,\n",
       " 'length_penalty': 1.0,\n",
       " 'no_repeat_ngram_size': 0,\n",
       " 'encoder_no_repeat_ngram_size': 0,\n",
       " 'bad_words_ids': None,\n",
       " 'num_return_sequences': 1,\n",
       " 'output_scores': False,\n",
       " 'return_dict_in_generate': False,\n",
       " 'forced_bos_token_id': None,\n",
       " 'forced_eos_token_id': None,\n",
       " 'remove_invalid_values': False,\n",
       " 'exponential_decay_length_penalty': None,\n",
       " 'suppress_tokens': None,\n",
       " 'begin_suppress_tokens': None,\n",
       " 'architectures': ['BertForMaskedLM'],\n",
       " 'finetuning_task': None,\n",
       " 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
       " 'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       " 'tokenizer_class': None,\n",
       " 'prefix': None,\n",
       " 'bos_token_id': None,\n",
       " 'pad_token_id': 0,\n",
       " 'eos_token_id': None,\n",
       " 'sep_token_id': None,\n",
       " 'decoder_start_token_id': None,\n",
       " 'task_specific_params': None,\n",
       " 'problem_type': None,\n",
       " '_name_or_path': 'google-bert/bert-base-uncased',\n",
       " 'transformers_version': '4.44.2',\n",
       " 'gradient_checkpointing': False,\n",
       " 'model_type': 'bert',\n",
       " 'vocab_size': 30522,\n",
       " 'hidden_size': 768,\n",
       " 'num_hidden_layers': 12,\n",
       " 'num_attention_heads': 12,\n",
       " 'hidden_act': 'gelu',\n",
       " 'intermediate_size': 3072,\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'attention_probs_dropout_prob': 0.1,\n",
       " 'max_position_embeddings': 512,\n",
       " 'type_vocab_size': 2,\n",
       " 'initializer_range': 0.02,\n",
       " 'layer_norm_eps': 1e-12,\n",
       " 'position_embedding_type': 'absolute',\n",
       " 'use_cache': True,\n",
       " 'classifier_dropout': None}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "config_dict = config.to_dict()\n",
    "config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d3bfefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEncoder(\n",
       "  (layer): ModuleList(\n",
       "    (0-11): 12 x BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSdpaSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (intermediate_act_fn): GELUActivation()\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "16670785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertAttention(\n",
       "  (self): BertSdpaSelfAttention(\n",
       "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (output): BertSelfOutput(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.encoder.layer[0].attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1225b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
